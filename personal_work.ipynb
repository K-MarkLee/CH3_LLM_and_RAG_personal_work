{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **개인 프로젝트 파일**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. 사용환경 준비**\n",
    "### **1.1. 패키지**\n",
    "\n",
    "프로그램에 필요한 패키지와 클래스들을 모두 불러오는 단계이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 환경 변수를 불러오기 위한 os 모듈을 사용한다.\n",
    "import os\n",
    "\n",
    "# re는 정규 표현식(Regular Expression) 관련 작업을 처리하는 모듈이다.\n",
    "# re.sub는 특정 패턴에 해당하는 문자열을 다른 문자열로 치환하거나 삭제하는 기능을 제공한다.\n",
    "import re\n",
    "\n",
    "# FAISS(Facebook AI Similarity Search)는 벡터 검색과 유사도 매칭을 위한 라이브러리이다.\n",
    "import faiss\n",
    "\n",
    "# similarity score 평균 계산을 위해 Python 내장 통계 모듈의 mean 함수를 사용한다.\n",
    "from statistics import mean\n",
    "\n",
    "# LangChain 패키지에서 OpenAI 기반의 채팅 모델을 활용하기 위한 ChatOpenAI 클래스를 불러온다.\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LangChain 패키지에서 OpenAI 임베딩을 생성하기 위한 OpenAIEmbeddings 클래스를 불러온다.\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# LangChain 패키지에서 텍스트를 분리하기 위한 도구인 CharacterTextSplitter와 RecursiveCharacterTextSplitter를 불러온다.\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# LangChain 커뮤니티 패키지에서 PDF 파일을 로드하기 위한 PyPDFLoader 클래스를 불러온다.\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 벡터 데이터베이스를 생성하기 위해 FAISS 클래스와 InMemoryDocstore 클래스를 불러온다.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# LangChain을 이용하여 RAG(Retrieval-Augmented Generation) 체인을 생성하는 데 필요한 ChatPromptTemplate 및 RunnablePassthrough 클래스를 불러온다.\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 벡터 데이터베이스에 텍스트를 추가하기 위해 Document 클래스를 불러온다.\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# 고유한 ID(UUID)를 생성하기 위해 uuid4 함수를 사용한다.\n",
    "from uuid import uuid4\n",
    "\n",
    "# 도전과제에서의 프린트 시간을 생성하기 위한 기능 불러오기.\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **2. API 환경변수 설정 및 모델 초기화**\n",
    "\n",
    "API 키를 설정하고 모델의 초기화를 하는 단계이다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI_API_KEY에 현재 나의 GPT_API키의 값을 가져와서 할당한다.\n",
    "os.environ[\"OpenAI_API_KEY\"] = os.getenv(\"GPT_API\")\n",
    "\n",
    "# gpt-4o 모델의 초기화.\n",
    "model = ChatOpenAI(model =\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **3. PDF 파일 로드 및 모델 초기화**\n",
    "\n",
    "PDF 파일을 로드하고, 페이지 별로 문서를 로드하는 단계이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 파일 로드. 파일의 경로 입력 \n",
    "# pdf는 과제 파일/ pdf1은 새로 추가한 파일이다.\n",
    "pdf = PyPDFLoader(\"data/LLM_Research_Trends.pdf\")\n",
    "pdf1 = PyPDFLoader(\"data/Research_Trends_in_LLM_and_Mathematical_Reasoning.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "pdf_loader = pdf.load()\n",
    "pdf1_loader = pdf1.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **3.1. 데이터 파악 밎 천처리**\n",
    "\n",
    "PDF 파일을 살펴보면, 페이지 내에 필요 없는 단어나 꾸밈 요소가 많이 포함되어 있다.  \n",
    "  \n",
    "이러한 불필요한 요소들은 학습의 효율성을 저하시킬 수 있으므로,  \n",
    "데이터 전처리를 통해 제거한다.\n",
    "\n",
    "---\n",
    "\n",
    "#### **전처리 대상**\n",
    "\n",
    "1. **각주와 주석**  \n",
    "   - 본문에 불필요한 추가 정보를 포함하는 각주 및 주석.\n",
    "\n",
    "2. **각종 URL**  \n",
    "   - 텍스트 내 포함된 웹사이트 주소 및 링크.\n",
    "\n",
    "3. **각종 첨부 문자**  \n",
    "   - 파일 내 특정 문자나 기호.\n",
    "\n",
    "4. **PDF 특성으로 인한 줄넘김**  \n",
    "   - PDF 형식으로 인해 생기는 부자연스러운 줄바꿈.\n",
    "\n",
    "5. **중복된 공백**  \n",
    "   - 연속된 공백으로 인해 텍스트가 불필요하게 길어지는 문제.\n",
    "\n",
    "6. **레퍼런스**  \n",
    "   - 학술적 참고자료 목록.\n",
    "\n",
    "---\n",
    "\n",
    "#### **추가 전처리 사항**\n",
    "- 새로운 파일 추가와 함께, 특정 문자의 제거 작업을 추가로 진행.\n",
    "\n",
    "---\n",
    "\n",
    "### **데이터 전처리 방법**\n",
    "- 정규식을 이용한 데이터의 분별 후 제거및 다른 데이터로 교체 할 예정.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 줄바꿈 제거 및 각주/참고문헌 제거 함수\n",
    "def clean_text(documents):\n",
    "    for doc in documents:\n",
    "        content = doc.page_content\n",
    "         \n",
    "        # 각주 제거: [숫자], [숫자, 숫자], [숫자-숫자] 형태\n",
    "        content = re.sub(r\"\\[\\d+(?:,\\s*\\d+)*\\]\", \"\", content)\n",
    "        \n",
    "        # [숫자 숫자 , 숫자 숫자] 또는 [숫자, 숫자] 패턴 제거\n",
    "        content = re.sub(r\"\\[\\s*(\\d+\\s*,?\\s*)+\\]\", \"\", content)\n",
    "        \n",
    "        # 특정 패턴 제거: 1), 23), 등\n",
    "        content = re.sub(r\"\\d+\\)\", \"\", content)\n",
    "        \n",
    "        \n",
    "        # 링크 제거: http://, https://, www. 등으로 시작하는 URL\n",
    "        content = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", content)\n",
    "\n",
    "        # 불필요한 줄바꿈 제거 (문장 끝은 유지)\n",
    "        content = re.sub(r\"(?<![.!?])\\n\", \"\", content)\n",
    "\n",
    "        # 중복 공백 제거\n",
    "        content = re.sub(r\"\\s{2,}\", \" \", content)\n",
    "        \n",
    "\n",
    "        # 특정 텍스트의 제거\n",
    "        content = re.sub(r\"참고문헌.*\", \"\", content, flags=re.DOTALL) # 참고문헌과 그 이후의 데이터 전부 제거     \n",
    "        content = re.sub(r\"2023. 11 정보과학회지.\\d+\\s\", \"\", content) # 특정 문구과 그 뒤의 숫자 제거\n",
    "        content = re.sub(r\"\\d+\\s*특집원고 초거대 언어모델 연구 동향\", \"\", content) # 특정 문구와 그 앞의 숫자 제거\n",
    "        content = re.sub(r\"\\d+\\s*권오욱 외 / 초거대 언어모델과 수학추론 연구 동향\", \"\", content)\n",
    "        content = re.sub(r\"특집원고\",\"\",content)\n",
    "        \n",
    "        \n",
    "        # 정리된 텍스트 저장\n",
    "        doc.page_content = content.strip()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **3.2. 전처리 과정을 거친 데이터**\n",
    "\n",
    "전처리 함수를 통해 PDF 파일의 데이터를 정제합니다.  \n",
    "정제된 데이터는 `cleaned_doc` 객체에 저장되며, 이후의 추가 처리 작업에 활용됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### **전처리 과정**\n",
    "1. PDF 파일을 전처리 함수(`clean_text`)에 입력합니다.\n",
    "2. 함수는 필요 없는 각주, URL, 중복 공백, 특정 패턴 등을 제거합니다.\n",
    "3. 전처리된 데이터를 `cleaned_doc`에 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pdf = clean_text(pdf_loader)\n",
    "cleaned_pdf1 = clean_text(pdf1_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **4. 문서 청크로 나누기**\n",
    "\n",
    "### **4.1 CharacterTextSplitter**\n",
    "\n",
    "`CharacterTextSplitter`는 고정된 기준을 가지고 텍스트를 분할하는 도구입니다.  \n",
    "아래는 각 주요 매개변수와 그 동작에 대한 설명입니다.\n",
    "\n",
    "---\n",
    "\n",
    "**1. `separator`**\n",
    "- 텍스트를 분할할 때 사용하는 기준을 지정합니다.\n",
    "- 예: `\"\\n\\n\"`은 줄바꿈이 두 번 일어난 부분을 기준으로 텍스트를 분리합니다.  \n",
    "  - 이는 문단 단위의 분할을 의미합니다.\n",
    "- PDF의 특성상 `\"\\n\"`은 문장마다 줄바꿈이 되어 있고, `\"\\n\\n\"`은 대체로 한 페이지를 의미합니다.\n",
    "\n",
    "---\n",
    "\n",
    " **2. `chunk_size`**\n",
    "- 분할된 각 조각의 **최대 길이**를 설정합니다.\n",
    "- 이상하게 동작하는 경우가 있다.\n",
    "  - 조각의 길이가 100을 넘는데도 허용되거나,\n",
    "  - `\"\\n\"` 기준으로 문장이 분리되어 각 조각에 단어가 적음에도 불구하고 `chunk_size` 초과 에러가 발생.\n",
    "- 이로 인해 **각 조각의 최대 길이**라기보다는, **총 조각의 최대 개수**로 작동하는 것이 아닌가 추측됩니다.\n",
    "\n",
    "---\n",
    "\n",
    " **3. `chunk_overlap`**\n",
    "- 분할된 조각 간의 **중복 길이**를 설정합니다.\n",
    "- 문제점:\n",
    "  - 페이지마다 분할이 이루어지는 경우, 앞뒤 내용이 짤리지만 `overlap`이 적용되지 않음.\n",
    "  - 이는 데이터를 **페이지 단위로 분할**한 방식 때문으로 보입니다.\n",
    "\n",
    "---\n",
    "\n",
    " **4. `length_function`**\n",
    "- 텍스트의 길이를 측정하는 방식을 설정합니다.\n",
    "- 기본값은 Python의 `len()` 함수로, 문자열의 길이를 기준으로 측정합니다.\n",
    "- 필요에 따라 다른 함수(예: 단어 수 계산)를 정의해 사용할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    " **5. `is_separator_regex`**\n",
    "- `separator`가 정규식(Regular Expression)으로 처리될 수 있는지 여부를 설정합니다.\n",
    "- `True`로 설정하면 `separator`에 정규식을 사용하여 텍스트를 분리할 수 있습니다.\n",
    "  - 예: `separator=r\"\\d+\\.\"`은 숫자와 점(`.`)을 기준으로 분리합니다.\n",
    "\n",
    "---\n",
    "\n",
    " **요약**\n",
    "`CharacterTextSplitter`는 텍스트 분할을 위한 강력한 도구지만, PDF와 같은 특정 형식에서는 예상과 다르게 동작할 수 있습니다. 이는 PDF의 줄바꿈 구조나 데이터 로드 방식의 특수성 때문일 가능성이 큽니다.\n",
    "\n",
    "- `separator`: 분리 기준.\n",
    "- `chunk_size`: 각 조각의 최대 길이.\n",
    "- `chunk_overlap`: 조각 간 중복 길이.\n",
    "- `length_function`: 텍스트 길이 측정 방식.\n",
    "- `is_separator_regex`: 정규식 분리 여부.\n",
    "\n",
    "PDF 데이터의 특수한 요구 사항에 따라 설정값을 조정하거나, 추가적인 전처리 과정을 도입하는 것이 필요할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **4.2. RecursiveCharacterTextSplitter**\n",
    "\n",
    "`RecursiveCharacterTextSplitter`는 `CharacterTextSplitter`보다 **더 유연한 텍스트 분할**을 지원하는 도구입니다.\n",
    "\n",
    "---\n",
    "\n",
    " **주요 특징**\n",
    "1. **`separators`**를 사용하여 분할 기준을 여러 개 설정할 수 있습니다.\n",
    "   - `separators`는 리스트 형태로 제공되며, 리스트에 있는 분할 기준은 **우선순위**에 따라 적용됩니다.\n",
    "   - 예를 들어:\n",
    "   \n",
    "     ```python\n",
    "     separators = [\"\\n\\n\", \"\\n\", \" \"]\n",
    "     ```\n",
    "     위의 설정은 먼저 `\"\\n\\n\"`을 기준으로 분할을 시도하고, 실패하면 `\"\\n\"`을 기준으로, 마지막으로 `\" \"`(공백)을 기준으로 분할합니다.\n",
    "\n",
    "2. 나머지 매개변수(`chunk_size`, `chunk_overlap`, `length_function`, `is_separator_regex`)는 `CharacterTextSplitter`와 동일하게 동작합니다.\n",
    "\n",
    "---\n",
    "\n",
    " **`RecursiveCharacterTextSplitter`와 `CharacterTextSplitter`의 차이점**\n",
    "| **특징**                        | **CharacterTextSplitter**                       | **RecursiveCharacterTextSplitter**              |\n",
    "|---------------------------------|-------------------------------------------------|------------------------------------------------|\n",
    "| **분할 기준**                  | 단일 기준 (`separator`) 사용                   | 다중 기준 (`separators` 리스트) 사용            |\n",
    "| **우선순위 분할**              | 없음                                           | 리스트의 순서대로 분할 시도                     |\n",
    "| **유연성**                     | 고정된 분할 기준                              | 여러 기준을 순차적으로 시도하며 더 유연         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"] ,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3. 텍스트 분할 및 레퍼런스 제외**\n",
    "\n",
    "전처리된 텍스트(`cleaned_doc`)를 `text_splitter`를 사용해 분할하여 `text_splits` 객체에 저장한다.  \n",
    "pdf 의 경우 11페이지부터는 레퍼런스가 포함되어 있으므로, **10페이지까지만** 데이터를 불러온다.\n",
    "\n",
    "pdf 의 경우에는 10페이지부터 있기 때문에, **9페이지까지만** 데이터를 불러온다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pdf_text_splits = text_splitter.split_documents(cleaned_pdf)\n",
    "pdf_text_splits = pdf_text_splits[:11]\n",
    "\n",
    "pdf1_text_splits = text_splitter.split_documents(cleaned_pdf1)\n",
    "pdf1_text_splits = pdf1_text_splits[1:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **Recursive 테스트**\n",
    "\n",
    "`RecursiveCharacterTextSplitter`를 사용해 테스트를 진행합니다.  \n",
    "`merge_lines` 함수를 적용하지 않는 이유는, **recursive 방식은 한 줄씩 스플릿되기 때문에** 계단식으로 데이터가 증가하는 현상이 발생하기 때문입니다.  \n",
    "또한, `RecursiveCharacterTextSplitter`는 추가적인 조정이 많이 필요해 보이며, 테스트 이후에는 `text_splits` 방식으로 데이터 처리를 이어갈 계획입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_recursive = recursive_text_splitter.split_documents(cleaned_pdf)\n",
    "\n",
    "# 빈 리스트를 생성\n",
    "filter_recursive = []\n",
    "\n",
    "for doc in text_recursive:\n",
    "    # 11페이지 까지니까 10과 같거나 작아야한다.\n",
    "    if doc.metadata.get('page') <= 10:\n",
    "        \n",
    "        # 나오는 doc을 새로운 리스트에 할당.  \n",
    "        filter_recursive.append(doc)  \n",
    "        # 여기서 filter_recursive += doc하게되면 객체에 따로따로 들어가기 떄문에 변경.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **4.4. merge_lines 함수**\n",
    "\n",
    "`merge_lines` 함수는 PDF의 특성상 문장이 페이지를 넘어가면서 끊기는 문제를 해결하기 위해 만들어졌습니다.  \n",
    "텍스트 분할 결과(`text_splits`)를 조정하여, **이전 청크의 마지막 줄을 현재 청크의 앞부분에 연결**합니다.\n",
    "\n",
    "---\n",
    "\n",
    " **필요성**\n",
    "- PDF에서 문장이 페이지를 넘어갈 때 끊기는 경우가 발생.\n",
    "- `chunk_overlap`이 정상적으로 동작하지 않아 추가적인 처리가 필요.\n",
    "- 이전 청크의 마지막 줄을 현재 청크의 앞부분에 연결하여 문장의 단절 문제를 해결.\n",
    "\n",
    "---\n",
    "\n",
    " **구현 순서**\n",
    "1. **텍스트 갯수 확인 및 반복 설정**\n",
    "   - `text_splits`의 길이만큼 반복문 실행.\n",
    "\n",
    "2. **첫 번째 청크 건너뛰기**\n",
    "   - 첫 번째 텍스트(`i == 0`)는 이전 줄이 없으므로 건너뜀.\n",
    "\n",
    "3. **이전 청크의 내용 가져오기**\n",
    "   - 이전 청크(`i-1`)의 내용을 `prev_content`에 할당.\n",
    "\n",
    "4. **이전 청크의 마지막 줄 추출**\n",
    "   - `prev_content.split(\"\\n\")[-1]`로 이전 청크의 마지막 줄을 `last_line`에 저장.\n",
    "\n",
    "5. **현재 청크의 내용 가져오기**\n",
    "   - 현재 텍스트 데이터를 `text_splits[i].page_content`에서 가져옴.\n",
    "\n",
    "6. **텍스트 연결**\n",
    "   - 이전 텍스트 데이터를 현재 텍스트 데이터 앞에 붙임.\n",
    "\n",
    "7. **결과 업데이트**\n",
    "   - 연결된 텍스트로 현재 청크의 내용을 덮어씀.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lines(text):\n",
    "    \n",
    "    # 첫 번째 청크는 제외하기\n",
    "    for i in range(1, len(text)):\n",
    "        \n",
    "        # 이전 청크의 마지막 줄 추출\n",
    "        previous_split = text[i - 1].page_content\n",
    "        \n",
    "        # 마지막줄 추출하기\n",
    "        last_line = previous_split.strip().splitlines()[-1]\n",
    "\n",
    "        # 현재 청크의 텍스트 가져오기\n",
    "        current_split = text[i].page_content\n",
    "\n",
    "        # 이전 청크의 마지막 줄과 현재 청크의 첫 번째 줄을 연결\n",
    "        merged_text = last_line + \" \" + current_split.strip()\n",
    "\n",
    "        # 업데이트된 내용을 현재 청크에 저장\n",
    "        text[i].page_content = merged_text\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **4.5. 텍스트 병합 및 데이터 확인**\n",
    "\n",
    "`text_splits` 객체(스플릿된 텍스트 데이터)를 `merge_lines` 함수를 통해 처리하여 페이지 간 끊어진 문장을 연결한 새 객체 `merge_splits`에 저장합니다.  \n",
    "이를 표시하여 데이터가 정제된 결과를 확인합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf file content : \n",
      "스플릿 0:\n",
      "page_content='초거대 언어모델 연구 동향업스테이지 박찬준*･이원성･김윤기･김지후･이활석 1. 서 론ChatGPT와 같은 초거대 언어모델(Large Language Model, LLM) 의 등장으로 기존에 병렬적으로 연구되던 다양한 자연언어처리 하위 분야들이 하나의 모델로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의응답, 형태소분석 등의 작업을 모두 처리할 수 있게 되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이에 따라 사용자의 목적에 맞는 출력을 생성하는 패러다임을 맞이하게 되었다 .\n",
      "LLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 요인에 기반하고 있으며, 이 요인들은 현대 자연언어처리 (Natural Language Processing, NLP) 연구의 핵심적인 추세로 간주된다. 첫째로, 데이터의 양적 확대는 무시할 수 없는 중요한 요인이다. 디지털화의 선도로, 텍스트 데이터의 양이 기하급수적으로 증가하였고, 이는 연구의 질적 변화를 가져왔다. 대규모 코퍼스의 활용은 LLM의 일반화 능력을 향상시키며, 다양한 맥락과 주제에 대한 깊은 학습을 가능하게 한다. 둘째로, 컴퓨팅 기술의 진보는 LLM의 발전에 있어 결정적이었다. 특히, Graphics Processing Unit (GPU) 및 Tensor Processing Unit (TPU) 와 같은 고성능 병렬 처리 하드웨어의 개발은 모델 학습에 있어 병목 현상을 크게 완화시켰다. 이로 인해 연구자들은 모델의 복잡성을 키우고, 더욱 깊은 신경망 구조를 탐구할 수 있게 되었다. 셋째, 알고리즘 및 기술의 발전은 LLM의 성능 향상을 주도하였다. Attention 및 Transformer Architecture의 도입은 연구자들에게 문맥 간의 관계를 더욱 정교하게 모델링할 수 있는 방법을 제공하였다 . 이 모든 변화의 중심에는 ‘scaling law’라는 * 정회원 학문적인 통찰이 있다 . 해당 연구에 따르면, 모델의 크기와 그 성능은 긍정적인 상관 관계를 보인다. 이를 통해 연구자들은 모델의 파라미터 수를 증가시키면서, 이에 따른 성능 향상을 기술적 진보의 상호 작용에서 나온 결과이며, 이러한 추세는 앞으로도 NLP 연구의 주요 동력이 될 것으로 예상된다.\n",
      "연구단계를 넘어 LLM은 산업계에서도 많은 발전을 이루어 내고 있다. LLM 은 교육, 의료, 금융, 제조 등 거의 모든 산업 분야에서 광범위한 활용 가능성을 제시하고 있다 . 교육 분야에서는 단순한 정보 검색을 넘어, 개인화된 학습 경로를 추천하는 시스템, 과제의 자동 평가, 학생들의 복잡한 질문에 대한 답변 제공 등의 역할로 활용될 수 있다. 이는 교육의 효율성과 개인화를 동시에 추구하는 현대의 교육 트렌드와 맞물려 큰 효과를 발휘할 것으로 기대된다. 의료 분야에서는 환자 데이터를 기반으로 한 초기 진단 도구로 활용될 뿐만 아니라, 복잡한 의료 기록 분석, 신약 개발에 필요한 연구 데이터 분석, 또는 최신 의학 연구 동향 파악 등의 다양한 역할을 수행할 수 있다. 이로써 의료 전문가들의 결정을 보조하고, 효율적인 치료 방향을 도모할 수 있게 된다. 금융 분야에서는 개인의 투자 성향과 시장의 동향을 분석하여 투자 권고를 제공하는 것 외에도, 금융 위험을 상세하게 분석하거나, 복잡한 금융 거래를 자동화하는 시스템의 핵심 구성 요소로서의 역할을 할 수 있다. 이는 금융 서비스의 효율과 안전성 향상에 크게 기여할 것이다. 제조 분야에서도 LLM은 설계 단계부터 생산, 품질 관리에 이르기까지의 전 과정에서 데이터 분석 및 최적화 도구로 활용될 수 있다. 생산 효율성 향상과 제품 품질 향상을 도모하며, 고객의 니즈에 더욱 민첩하게 대응할 수 있는 기회를 제공한다.\n",
      "그러나, 이러한 긍정적인 측면들과 더불어 LLM의 한계점과 위험성도 고려되어야 한다. LLM 은 학습 데이터의 편향성을 그대로 반영할 수 있어, 편향된 결과나 추천을 할 가능성이 있다 . 이는 특히 중요한 의' metadata={'source': 'data/LLM_Research_Trends.pdf', 'page': 0}\n",
      "\n",
      " {'--------------------------------------------------'} \n",
      "\n",
      "pdf1 file contetn : \n",
      "스플릿 0:\n",
      "page_content='2전자통신동향분석 제38권 제6호 2023년 12월유용성이 널리 알려졌다. 초거대 언어모델의 능력과 일반화에 대한 유용성은 GPT-3에서부터 언급되었지만, 학습된 정보로부터 새로운 정보를 이끌어낼 수 있는 추론 능력에 있어서는 일반화의 가능성을 보여줄 정도로 뛰어나지 않았다. 특히, 서술형 수학추론에서 성능이 낮았는데, 이후 ChatGPT 와 GPT-4로 발전하면서 서술형 수학추론 능력 또한 비약적으로 발전하였다. 본고에서는 초거대 언어모델과 이를 개선하기 위한 연구 동향, 그리고 인공지능의 추론 능력 검증에 적합한 수학추론 연구 동향에 대해 살펴보고자 한다.\n",
      "Ⅱ. 초거대 언어모델 최근 동향1.\t초거대\t언어모델\t개발\t동향언어모델이란 ‘특정 문장이 등장할 확률을 계산해 주는 모델’로 전산언어학에서 시작된 개념이지만, 딥러닝 전이학습(T ransfer Learning )과 결합하여 BERT, GPT-3 공개 이후 인공지능 기술의 중요한 축으로 자리 잡았고, 언어모델의 매개변수(Param-eter) 크기를 늘리는 형태의 초거대 언어모델(LLM: Large Language Models) 구축 경쟁이 촉발되었다. LLM은 언어모델의 한 종류인 언어 ‘생성’ 모델을 방대한 데이터, 수백억 개 매개변수, 이를 학습하기에 충분한 컴퓨팅 자원을 기반으로 계산량을 크게 키워서 다음 단어 예측 정확도를 획기적으로 향상한 모델을 말한다. 이러한 규모의 학습을 통해 구축된 초거대 언어모델은 사전학습(Pre-training) 후에 미세조정(Fine-tuning)이 필요한 기존 언어모델과 다르게 10여 개의 소량 학습데이터로 응용 태스크에 적용이 가능한 few-shot 학습, 추가 학습이 필요없이 단지 태스크에 대한 설명 문구를 지정해 주는 것만으로도 응용 태스크에 적용이 가능한 zero-shot 학습을 가능하게 했다. Few-shot 학습과 zero-shot 학습처럼 추가학습 없이 입력 컨텍스트로만 응용 태스크에 적용 가능한 학습을 ICL(In-Context Learning)이라고 한다.\n",
      "LLM으로 사람 수준의 문장 생성이 가능하고 소량의 학습 데이터로도 응용 태스크 적용이 가능하여 범용 인공지능(Artificial General Intelligence)에 한걸음 전진했지만, 모델 학습과 구축에 너무 많은 컴퓨팅 비용이 소요되고 또한 실서비스에서도 비용 문제가 걸림돌이 된다. 본격적인 LLM 인 OpenAI 의 GPT -3 언어모델의 성능은 모델의 구조보다는 매개변수, 데이터, 컴퓨팅 파워에 좌우된다는 Scaling Law를 근거로 175B 개 매개변수를 갖는 초거대 생성모델을 학습하여 2020 년에 공개하였고, 이에 바탕을 둔 ChatGPT를 2022년 말에 공개하여 일반인에게 그 유용성을 인정받아 많은 기업 및 연구소에서 초거대모델 개발 경쟁에 뛰어들게 하는 계기가 되었다. 이러한 추세에서 구글은 매개변수를 늘려가면서 학습했을 때 상대적으로 작은 개수(8B)의 매개변수 언어모델에 비해 초거대에 해당하는 언어모델(540B)에 갑자기 많은 태스크를 수행할 수 있다는 Emergent Abilities라는 개념을 근거로 거대 모델의 유용성을 주장하였다.' metadata={'source': 'data/Research_Trends_in_LLM_and_Mathematical_Reasoning.pdf', 'page': 1}\n",
      "\n",
      " {'--------------------------------------------------'} \n",
      "\n",
      "recursive file contetn : \n",
      "스플릿 0:\n",
      "page_content='초거대 언어모델 연구 동향업스테이지 박찬준*･이원성･김윤기･김지후･이활석 1. 서 론ChatGPT와 같은 초거대 언어모델(Large Language Model, LLM) 의 등장으로 기존에 병렬적으로 연구되던 다양한 자연언어처리 하위 분야들이 하나의 모델로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의응답, 형태소분석 등의 작업을 모두 처리할 수 있게 되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이에 따라 사용자의 목적에 맞는 출력을 생성하는 패러다임을 맞이하게 되었다 .' metadata={'source': 'data/LLM_Research_Trends.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# merge lines를 통과한 pdf데이터를 생성.\n",
    "pdf_merge_splits = merge_lines(pdf_text_splits)\n",
    "print(\"pdf file content : \")\n",
    "# 각 청크마다 숫자를 매기고 \\n 으로 다음줄로 그 content를 소환.\n",
    "for idx, split in enumerate(pdf_merge_splits[:1]):\n",
    "    print(f\"스플릿 {idx}:\\n{split}\")\n",
    "\n",
    "# 중간 띄워쓰기    \n",
    "print(\"\\n\",{\"-\" * 50},\"\\n\")\n",
    "\n",
    "# pdf1 파일로도 생성\n",
    "pdf1_merge_splits = merge_lines(pdf1_text_splits)\n",
    "print(\"pdf1 file contetn : \")\n",
    "for idx, split in enumerate(pdf1_merge_splits[:1]):\n",
    "    print(f\"스플릿 {idx}:\\n{split}\")\n",
    "    \n",
    "\n",
    "print(\"\\n\",{'-' * 50},\"\\n\")\n",
    "    \n",
    "# recursive 파일로도 생성\n",
    "merge_recursive = merge_lines(filter_recursive)\n",
    "print(\"recursive file contetn : \")\n",
    "for idx, split in enumerate(filter_recursive[:1]):\n",
    "    print(f\"스플릿 {idx}:\\n{split}\")\n",
    "\n",
    "\n",
    "# # lambda x 는 score를 대변하며, 1에 가까울수록 정렬후, reverse = True로 변경 한 것이다.\n",
    "# sorted_results = sorted(results_with_scores, key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **5. LLM 모델 적용을 위한 임베딩 생성**\n",
    "\n",
    "LLM 모델에 데이터를 적용하기 위해 텍스트 데이터를 **임베딩(Embedding)** 해야 합니다.  \n",
    "임베딩 객체를 생성하고, 필요한 모델을 설정합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 임베딩 모델로 벡터 임베딩을 생성\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **6. 벡터 스토어 생성**\n",
    "\n",
    "임베딩 객체를 생성하고, 데이터를 벡터화합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 인덱스 생성\n",
    "# L2(유클리드 거리) 기반으로 텍스트 벡터를 저장하고 검색하기 위한 인덱스\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"LLM에 대해서\")))\n",
    "\n",
    "# FAISS 벡터스토어 생성\n",
    "vectorstore = FAISS(\n",
    "    # 임베딩 함수 지정: 텍스트를 벡터로 변환하는 함수\n",
    "    embedding_function=embeddings,\n",
    "    \n",
    "    # 인덱스 객체: 벡터의 저장 및 검색에 사용\n",
    "    index=index,\n",
    "    \n",
    "    # 문서 저장소: 메모리에 문서를 저장하여 인덱스와 연결\n",
    "    docstore=InMemoryDocstore(),\n",
    "    \n",
    "    # 문서와 인덱스 간 매핑: 각 문서의 ID와 벡터 인덱스 간 관계를 저장하는 딕셔너리\n",
    "    index_to_docstore_id={}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **6.1. 백터 데이터베이스에 문서 추가**\n",
    "\n",
    "uuid를 불러와 unique한 id를 생성하고 벡터 데이터베이스에 pdf파일을 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3c5e5497-a218-4c50-80e7-28de846ae7ad',\n",
       " '53195ca7-f40e-43b7-afbd-714dd848996c',\n",
       " 'bd547cda-d2db-467a-bc9b-b754436d82f4',\n",
       " 'faea7d17-e1be-4546-8a36-fc435759c585',\n",
       " 'dd74e7d8-d738-44fd-9416-48124de60a9e',\n",
       " '66b32408-87b9-4974-aec8-d9f6dd5cf177',\n",
       " '062ea4a0-39a4-44f7-902a-54d77637c7d8',\n",
       " 'fdbae48d-e9b3-4763-8391-a2da8d67bf71',\n",
       " '8e42f037-94c8-4367-86a0-6ba3fb21396a']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pdf 문서 데이터 추가\n",
    "# pdf_merge_splits의 갯수만큼, uuid 를 생성한다.\n",
    "uuids = [str(uuid4()) for _ in range(len(pdf_merge_splits))]\n",
    "\n",
    "# 벡터 데이터베이스에 각각의 문서에 id를 할당한다.\n",
    "vectorstore.add_documents(documents=pdf_merge_splits, ids = uuids)\n",
    "\n",
    "\n",
    "# pdf1 문서 데이터 추가\n",
    "uuids = [str(uuid4()) for _ in range(len(pdf1_merge_splits))]\n",
    "vectorstore.add_documents(documents=pdf1_merge_splits, ids = uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **6.2. 유사성 검사를 통한 문서 확인**\n",
    "\n",
    "문서가 둘다 제대로 들어가 있는지를 알기 위해\n",
    "유사성 검사를 통하여 제대로 content를 사용하는 지 확인.\n",
    "\n",
    "\n",
    "\n",
    "벡터 데이터베이스에서 텍스트와 가장 유사한 문서를 검색하고, 점수와 함께 검색 결과를 확인한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.354] [3. 한국어 초거대 언어모델 동향GPT , PALM 과 같은 대규모 ...] {'source': 'data/LLM_Research_Trends.pdf', 'page': 3}\n",
      "* [SIM=0.360] [일반적으로 LLM의 응용을 구현하기 위한 전체 학습-추론 파이프라인은 ( 사전학습, ...] {'source': 'data/Research_Trends_in_LLM_and_Mathematical_Reasoning.pdf', 'page': 3}\n",
      "* [SIM=0.362] [LLM으로 사람 수준의 문장 생성이 가능하고 소량의 학습 데이터로도 응용 ...] {'source': 'data/Research_Trends_in_LLM_and_Mathematical_Reasoning.pdf', 'page': 2}\n",
      "* [SIM=0.366] [그러나, 이러한 긍정적인 측면들과 더불어 LLM의 한계점과 위험성도 고려되어야 한다. ...] {'source': 'data/LLM_Research_Trends.pdf', 'page': 1}\n",
      "* [SIM=0.370] [4.3 활용 및 증강4.3.1 Utilization of LLMs해당 섹션에서는 LLM을 활용하는 ...] {'source': 'data/LLM_Research_Trends.pdf', 'page': 7}\n"
     ]
    }
   ],
   "source": [
    "# 기본 유사성 검색\n",
    "# results = vectorstore.similarity_search(\"LLM은 어떨가요?\", k=2, filter={\"source\": \"data/LLM_Research_Trends.pdf\"})\n",
    "# for res in results:\n",
    "#     print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "#############################################################\n",
    "\n",
    "# 점수와 함께 유사성 검색 (벡터 데이터베이스의 similarity_search_with_score을 사용)\n",
    "# k 가장 유사한 문서의 갯수를 뜻함. 5면 가장 유사한 데이터 5개를 뽑음.\n",
    "results_with_scores = vectorstore.similarity_search_with_score(\"LLM에 대해 이야기해주세요.\", k=5)\n",
    "\n",
    "# res는 검색된 문서의 객체, score는 거리이다. (점수가 낮아야 정확도 올라감)\n",
    "for res, score in results_with_scores:\n",
    "    \n",
    "    # page_content에서 첫 10단어만 가져오기\n",
    "    # \" \" 은 스페이스의 허용/ \"\"는 스페이스 없이\n",
    "    limited_content = \" \".join(res.page_content.split()[:10])  # 첫 10단어만 추출\n",
    "    print(f\"* [SIM={score:.3f}] [{limited_content} ...] {res.metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **7. FAISS를 Retriever로 변환**\n",
    "\n",
    "retriever생성한다.\n",
    "벡터 데이터베이스의 as_Retriever을 사용한다.\n",
    "\n",
    "검색의 방식을 \"유사도\" 로 설정하고,  \n",
    "검색 결과에서 가장 유사한 문서 5개를 반환하도록 설정한다.\n",
    "  \n",
    "이는 5개의 유사한 문서를 토대로 답변을 생성 할 것이다. (3~5 디폴트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **8. 프롬프트 템플릿의 정의**\n",
    "\n",
    "컨텍스트 데이터를 기반으로 질문에 답변을 생성하기 위한 **프롬프트 템플릿**이다.  \n",
    "이 프롬프트는 다음과 같은 특징을 가지고 있다.\n",
    "\n",
    "---\n",
    " **프롬프트의 구성**\n",
    "1. **시스템 메시지**:\n",
    "   - 답변은 제공된 컨텍스트 데이터만 사용하도록 제한.\n",
    "   - 질문의 입력 언어와 상관없이 **항상 한국어**로 답변 생성.\n",
    "   - 컨텍스트 데이터에 없는 질문일 경우, 관련된 질문을 다시 요청하도록 안내.\n",
    "\n",
    "2. **사용자 메시지**:\n",
    "   - 질문과 컨텍스트 데이터를 함께 입력.\n",
    "   - 컨텍스트와 질문을 분리하여 명확하게 전달.\n",
    "\n",
    "---\n",
    "\n",
    " **사용 목적**\n",
    "- 모델이 제공된 **컨텍스트 데이터** 내에서만 질문에 대한 답변을 생성하도록 유도.\n",
    "- 답변의 언어를 한국어로 고정하여 일관성을 유지.\n",
    "- 질문이 컨텍스트 데이터와 관련 없는 경우, 답변 대신 **다른 질문을 요청**함으로써 모델의 한계를 명확히 전달.\n",
    "\n",
    "\n",
    "이 프롬프트는 컨텍스트 데이터 기반의 응답 생성에 최적화되어 있으며, 제한된 범위 내에서 신뢰성 있는 답변을 보장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "#     # 시스템 메시지: 답변 규칙 정의\n",
    "#     (\"system\", \"Answer the following question using only given context data.\"),\n",
    "#     (\"system\", \"Answer must be given in Korean, regardless of the input language.\"),\n",
    "#     (\"system\", \"If the question is outside the given context data, suggest asking something related.\"),\n",
    "\n",
    "    \n",
    "#     (\"system\", \"Here is an example of how to answer a question:\\n\"\n",
    "#                \"User: LLM에 대해서 알려줘\\n\"\n",
    "#                \"AI: LLM은 대규모 언어 모델로, 자연어 처리 작업에 활용됩니다.\\n\"\n",
    "#                \"Follow this format when generating responses.\"),\n",
    "    \n",
    "    \n",
    "#     (\"system\", \"답변은 ~다. 로 끊지않고 친근한 말투를 사용해줘.\\n\"\n",
    "#                \"User: LLM에 대해서 알려줘\\n\"\n",
    "#                \"AI: LLM은 대규모 언어 모델로, 자연어 처리 작업에 활용되고 있어.\\n\"\n",
    "#                \"Follow this format when generating responses\"),\n",
    "    \n",
    "#     # 한 문장이 끝날 때 줄바꿈 요청\n",
    "#     (\"system\", \"답변은 반드시 한 문장이 끝날 때마다 줄바꿈('\\n')을 추가해서 작성해.\\n\"\n",
    "#                \"이 규칙을 모든 문장에 적용해.\\n\"\n",
    "#                \"예를 들어:\\n\"\n",
    "#                \"User: LLM에 대해서 알려줘\\n\"\n",
    "#                \"AI: LLM은 대규모 언어 모델을 의미해.\\n\"\n",
    "#                \"주로 자연어 처리 작업에 사용되며,\\n\"\n",
    "#                \"번역, 요약, 질의응답 등 다양한 작업을 수행할 수 있어.\\n\"\n",
    "#                \"프롬프트를 어떻게 입력하느냐에 따라,\\n\"\n",
    "#                \"다양한 능력을 발휘할 수 있지.\\n\"\n",
    "#                \"Follow this format when generating responses\"),\n",
    "    \n",
    "#     # 사용자 메시지 템플릿\n",
    "#     (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 도전 과제를 위한 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='(\"system\", \"Answer the following question using only given context data.\"),\\n(\"system\", \"Answer must be given in Korean, regardless of the input language.\"),\\n(\"system\", \"If the question is outside the given context data, suggest asking something related.\"),\\n'), additional_kwargs={}), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\"# Your role\\nYou are a brilliant expert at understanding the intent of the questioner and the crux of the question, and providing the most optimal answer to the questioner\\'s needs from the documents you are given.\\n\\n\\n# Instruction\\nYour task is to answer the question using the following pieces of retrieved context delimited by XML tags.\\n\\n<retrieved context>\\nRetrieved Context:\\n{context}\\n</retrieved context>\\n\\n\\n# Constraint\\n1. Think deeply and multiple times about the user\\'s question\\\\\\nUser\\'s question:\\\\\\n{question}\\\\\\nYou must understand the intent of their question and provide the most appropriate answer.\\n- Ask yourself why to understand the context of the question and why the questioner asked it, reflect on it, and provide an appropriate response based on what you understand.\\n2. Choose the most relevant content(the key content that directly relates to the question) from the retrieved context and use it to generate an answer.\\n3. Generate a concise, logical answer. When generating the answer, Do Not just list your selections, But rearrange them in context so that they become paragraphs with a natural flow. \\n4. When you don\\'t have retrieved context for the question or If you have a retrieved documents, but their content is irrelevant to the question, you should answer \\'I can\\'t find the answer to that question in the material I have\\'.\\n5. Use five sentences maximum. Keep the answer concise but logical/natural/in-depth.\\n\\n\\n# Question:\\n{question}\"'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Context: {context}\\\\n\\\\nQuestion: {question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파일 경로와 시스템 메시지 파일 리스트\n",
    "path = \"Prompts/\"\n",
    "\n",
    "# prompt 1은 디폴트 이며 2,3,4 를 따로 설정해뒀음.\n",
    "# system_files = [\"prompt1.txt\", \"prompt2.txt\", \"prompt3.txt\", \"prompt4.txt\"]\n",
    "system_files = [\"prompt1.txt\",\"prompt3.txt\"]\n",
    "# 시스템 메시지 불러오기\n",
    "system_messages = []\n",
    "for txt in system_files:\n",
    "    with open(os.path.join(path, txt), \"r\", encoding=\"UTF-8\") as f:\n",
    "        \n",
    "         # \\\\n을 \\n으로 변환\n",
    "        content = f.read().replace(\"\\\\n\", \"\\n\") \n",
    "        system_messages.append((\"system\", content))\n",
    "\n",
    "\n",
    "# 사용자 메시지 템플릿 추가\n",
    "system_messages.append((\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\"))\n",
    "\n",
    "# ChatPromptTemplate 생성\n",
    "contextual_prompt = ChatPromptTemplate.from_messages(system_messages)\n",
    "contextual_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **9. RAG 체인 구성**\n",
    "\n",
    "\n",
    "이 코드는 **RAG(Retrieval-Augmented Generation) 체인**의 디버깅과 문서 데이터를 모델에 적합한 형태로 변환하기 위해 설계되었습니다.  \n",
    "각 단계에서 데이터가 올바르게 전달되고 처리되는지 확인할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    " **1. `DebugPassThrough` 클래스**\n",
    "- 데이터를 전달하면서 디버깅을 수행하는 클래스.\n",
    "- **주요 역할**:\n",
    "  1. 입력 데이터를 그대로 전달.\n",
    "  2. 데이터가 정상적으로 전달되는지 확인 가능.\n",
    "- **특징**:\n",
    "  - `*args, **kwargs`를 사용해 유연하게 입력값을 처리.\n",
    "  - 테스트 시 출력(`print`)을 활성화하여 데이터 확인 가능.\n",
    "\n",
    "---\n",
    "\n",
    " **2. `ContextToText` 클래스**\n",
    "- **문서 리스트를 하나의 텍스트로 변환**하여 모델 입력에 적합한 형태로 가공.\n",
    "- **주요 역할**:\n",
    "  1. 입력된 문서 리스트(`inputs[\"context\"]`)의 각 문서(`page_content`)를 줄바꿈(`\\n`)으로 결합.\n",
    "  2. 컨텍스트(`context`)와 질문(`question`)을 분리하여 반환.\n",
    "\n",
    "  #### **2. ContextToText 클래스**\n",
    "\n",
    "`ContextToText` 클래스는 문서 리스트를 하나의 텍스트로 변환하여 모델 입력에 적합한 형태로 가공합니다.\n",
    "\n",
    "- **주요 기능**:\n",
    "  - 입력된 문서 리스트의 `page_content`를 줄바꿈(`\\n`)으로 결합.\n",
    "  - 결과를 `context`와 `question`으로 나누어 반환.\n",
    "\n",
    "---\n",
    "\n",
    " **3. DebugPassThrough 클래스**\n",
    "\n",
    "`DebugPassThrough` 클래스는 데이터를 디버깅하기 위해 설계된 도구입니다.  \n",
    "입력 데이터를 그대로 전달하면서, 필요 시 데이터를 확인할 수 있습니다.\n",
    "\n",
    "- **주요 기능**:\n",
    "  - 데이터를 전달하는 과정에서 디버깅 용도로 출력 가능.\n",
    "  - 데이터의 흐름을 점검하고, 전달 과정에서 발생할 수 있는 문제를 식별.\n",
    "\n",
    "- **특징**:\n",
    "  - `*args`와 `**kwargs`를 사용하여 유연하게 데이터를 처리.\n",
    "  - 테스트 시 출력(`print`)을 활성화하여 디버깅.\n",
    "\n",
    "---\n",
    "\n",
    " **4. RAG 체인 구성**\n",
    "\n",
    "RAG 체인은 검색, 디버깅, 데이터 변환, 프롬프트 생성, 모델 호출 단계를 포함하여 데이터의 흐름을 체계적으로 처리합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터가 정상적으로 전달되는지 확인을 위한 디버깅 클래스\n",
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    \n",
    "    # 어떠한 값이 올지 모르니 , *args, **kwargs를 사용하여 유기적으로 받을수 있도록 설정.\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        \n",
    "        # 프린트가 안이뻐 보여서 테스트 제외하고는 주석처리\n",
    "        # 받아오는 값을 그대로 출력한다.\n",
    "        # print(\"Debug Output:\", output)\n",
    "        return output\n",
    "    \n",
    "# 문서 리스트를 텍스트로 변환하는 단계 추가 (모델에 적합한 형태로 가공)\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    \n",
    "    def invoke(self, inputs, config=None, **kwargs):\n",
    "        \n",
    "        # 불러온 각 문서의 page_content를 출력하고 줄바꿈으로 결합하여 하나의 텍스트로 변경.\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        \n",
    "        # 출력할때, content와 question을 분리해서 반환한다.\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "    \n",
    "\n",
    "# RAG 체인에서 각 단계마다 DebugPassThrough 추가\n",
    "rag_chain_debug = {\n",
    "    \n",
    "    # 컨텍스트 문서에서 검색을 하는 retriever\n",
    "    \"context\": retriever,      \n",
    "    \n",
    "    # 유저의 인풋이 제대로 받는지 디버깅 과정을 통함              \n",
    "    \"question\": DebugPassThrough() \n",
    "    \n",
    "    # RAG체인의 순서를 설정. Retriever > Debug > context > prompt > model순으로 데이터 전달.      \n",
    "}  | DebugPassThrough() | ContextToText()|   contextual_prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **10. 챗봇 구동**\n",
    "\n",
    "\n",
    "이 코드는 **RAG(Retrieval-Augmented Generation)** 체인을 사용하여 질문에 답변을 생성하는 루프 기반 질의응답 시스템입니다.  \n",
    "사용자가 질문을 입력하면, 관련 문서를 검색하고 답변을 생성한 뒤, 검색된 문서의 유사도 점수를 계산합니다.\n",
    "\n",
    "---\n",
    "\n",
    " **1. 동작 흐름**\n",
    "\n",
    "1. **사용자 입력**:\n",
    "   - 사용자로부터 질문을 입력받습니다.\n",
    "   - **\"break\"** 입력 시 루프를 종료합니다.\n",
    "\n",
    "2. **RAG 체인 호출**:\n",
    "   - 사용자 입력(`query`)을 RAG 체인(`rag_chain_debug`)에 전달하여 답변을 생성합니다.\n",
    "\n",
    "3. **유사성 검색 및 점수 계산**:\n",
    "   - 입력된 질문과 가장 관련 있는 상위 5개의 문서를 검색(`similarity_search_with_score`).\n",
    "   - 검색된 문서들의 유사도 점수를 추출하고, 평균 점수를 계산합니다.\n",
    "\n",
    "4. **결과 출력**:\n",
    "   - 입력된 질문, 유사도 평균 점수, 그리고 최종 답변을 출력합니다.\n",
    "\n",
    "---\n",
    "\n",
    " **주요 기능 설명**\n",
    "\n",
    "1. **사용자 입력**:\n",
    "   - **`input`**을 통해 질문을 입력받습니다.\n",
    "   - **\"break\"**를 입력하면 루프를 종료하여 프로그램을 종료할 수 있습니다.\n",
    "\n",
    "2. **RAG 체인 호출**:\n",
    "   - **`rag_chain_debug.invoke(query)`**:\n",
    "     - 입력된 질문을 RAG 체인에 전달하여 검색된 문서를 기반으로 답변을 생성합니다.\n",
    "     - RAG 체인은 검색(`retriever`)과 데이터 변환(`ContextToText`) 및 LLM 호출을 포함한 파이프라인입니다.\n",
    "\n",
    "3. **유사성 검색 및 점수 계산**:\n",
    "   - **`vectorstore.similarity_search_with_score(query, k=5)`**:\n",
    "     - 질문과 가장 관련성이 높은 상위 5개의 문서를 검색.\n",
    "     - 검색 결과는 문서와 유사도 점수(`score`)로 반환됩니다.\n",
    "   - **유사도 점수 추출 및 평균 계산**:\n",
    "     - 검색된 문서의 유사도 점수만 추출하여 평균값을 계산(**`mean(scores)`**).\n",
    "     - 평균 점수는 검색된 문서와 질문 간의 관련성을 평가하는 척도입니다.\n",
    "\n",
    "4. **결과 출력**:\n",
    "   - **`print`**를 사용해 다음 정보를 출력합니다:\n",
    "     1. **`query`**: 사용자 입력 질문.\n",
    "     2. **`average_score`**: 검색된 문서와 질문 간 유사도 평균 점수.\n",
    "     3. **`response.content`**: RAG 체인이 생성한 최종 답변.\n",
    "\n",
    "---\n",
    "\n",
    " **요약**\n",
    "\n",
    "- **RAG 체인 기반 질의응답 시스템**:\n",
    "  - 사용자의 질문에 대해 관련 문서를 검색하고, 문서를 기반으로 LLM을 활용해 답변을 생성합니다.\n",
    "\n",
    "- **유사도 점수**:\n",
    "  - 검색된 문서와 질문 간의 관련성을 평가하기 위해 유사도 점수를 계산합니다.\n",
    "  - 평균 점수를 통해 검색된 결과의 신뢰성을 확인할 수 있습니다.\n",
    "\n",
    "- **간단한 루프 구조**:\n",
    "  - 프로그램이 사용자의 입력을 지속적으로 처리하며, **\"break\"** 입력 시 종료됩니다.\n",
    "  \n",
    "이 시스템은 사용자와의 상호작용을 통해 검색 및 생성 모델의 능력을 효과적으로 활용합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True: \n",
    "#     print(\"========================\")\n",
    "    \n",
    "#     # 사용자의 입력을 받음\n",
    "#     query = input(\"질문을 입력하세요 (break 입력시 종료): \")\n",
    "\n",
    "#     # \"break\" 입력 시 루프 종료\n",
    "#     if query.lower() == \"break\":\n",
    "#         break\n",
    "\n",
    "#     # 위에서 설정한 RAG chaing을 invoke즉 불로오고 값으로 유저의 인풋 query를 매개변수로 보냄.\n",
    "#     response = rag_chain_debug.invoke(query)\n",
    "    \n",
    "#     # 점수와 함께 유사성 검색 (상위 5개 문서)\n",
    "#     results_with_scores = vectorstore.similarity_search_with_score(query, k=5)\n",
    "    \n",
    "#     # 유사성 점수만 추출\n",
    "#     scores = [score for _, score in results_with_scores]\n",
    "\n",
    "#     # 평균 계산\n",
    "#     average_score = mean(scores)\n",
    "    \n",
    "#     print(\"Question : \", query)\n",
    "#     print(\"Distance : \", average_score)\n",
    "#     print(\"Final Response:\")\n",
    "#     print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 도전 과제를 위한 프린트 path로 저장하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "Question :  llm의 미래\n",
      "Distance :  0.34294936\n",
      "Final Response:\n",
      "초거대 언어모델(LLM)의 미래는 다양한 분야에서의 활용 가능성과 함께 지속적인 발전을 기대할 수 있습니다. LLM은 교육, 의료, 금융, 제조 등 거의 모든 산업 분야에서 개인화된 서비스와 효율성을 높이는 데 활용될 수 있습니다. 특히, 멀티모달 모델로의 확장과 더불어, 점점 더 복잡한 태스크를 수행할 수 있는 능력을 갖출 것으로 보입니다. 그러나, LLM의 발전과 함께 고비용 및 데이터 편향성, 안전성, 설명 가능성 등의 문제점도 해결해야 할 과제로 여겨지고 있습니다. 이러한 과제를 해결하기 위한 연구와 개발은 앞으로의 LLM 발전에 중요한 역할을 할 것입니다.\n",
      "Result saved to: Results/prompt3_result_20241118-121736.txt\n",
      "========================\n",
      "Question :  llm의 미래\n",
      "Distance :  0.34294936\n",
      "Final Response:\n",
      "LLM의 미래는 여러 측면에서 주목받고 있습니다. 먼저, LLM은 다양한 산업 분야에서 활용될 가능성을 보여주며, 교육, 의료, 금융, 제조 등에서 이미 그 활용도가 증가하고 있습니다. 그러나, LLM의 발전에는 데이터의 편향성, 안전성, 설명력 부족 등 해결해야 할 문제들이 여전히 존재합니다. 또한, 고성능의 LLM을 구축하는 데 필요한 막대한 비용과 자원은 지속적인 도전 과제가 될 것입니다. 이러한 문제들을 해결하면서도 LLM의 혜택을 극대화하기 위한 연구와 개발이 계속될 것이며, 향후에는 더욱 효율적이고 윤리적으로 사용될 수 있는 방향으로 발전할 것입니다.\n",
      "Result saved to: Results/prompt3_result_20241118-121753.txt\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# 결과 저장 경로 설정\n",
    "output_path = \"Results/\"\n",
    "\n",
    "# 현재 Results는 gitignore로 설정하여 없앴음.\n",
    "# 결과 저장 경로가 없으면 생성\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "while True: \n",
    "    print(\"========================\")\n",
    "    \n",
    "    # 사용자의 입력을 받음\n",
    "    query = input(\"질문을 입력하세요 (break 입력시 종료): \")\n",
    "\n",
    "    # \"break\" 입력 시 루프 종료\n",
    "    if query.lower() == \"break\":\n",
    "        break\n",
    "\n",
    "    # 위에서 설정한 RAG chain을 invoke 즉 불러오고 값으로 유저의 인풋 query를 매개변수로 보냄.\n",
    "    response = rag_chain_debug.invoke(query)\n",
    "    \n",
    "    # 점수와 함께 유사성 검색 (상위 5개 문서)\n",
    "    results_with_scores = vectorstore.similarity_search_with_score(query, k=5)\n",
    "    \n",
    "    # 유사성 점수만 추출\n",
    "    scores = [score for _, score in results_with_scores]\n",
    "\n",
    "    # 평균 계산 (가장 유사한 상위 5개의 청크의 거리의 평균)\n",
    "    average_score = mean(scores)\n",
    "    \n",
    "    # 출력 결과\n",
    "    print(\"Question : \", query)\n",
    "    print(\"Distance : \", average_score)\n",
    "    print(\"Final Response:\")\n",
    "    print(response.content)\n",
    "    \n",
    "    \n",
    "    ###################################### 동일\n",
    "    \n",
    "\n",
    "    # 파일 이름에 현재 시간 추가\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    if \"prompt2.txt\" in system_files:\n",
    "        file_name = f\"prompt2_result_{timestamp}.txt\"\n",
    "    elif \"prompt3.txt\" in system_files:\n",
    "        file_name = f\"prompt3_result_{timestamp}.txt\"\n",
    "    elif \"prompt4.txt\" in system_files:\n",
    "        file_name = f\"prompt4_result_{timestamp}.txt\"\n",
    "    else : \n",
    "        file_name = f\"prompt_result_{timestamp}.txt\"\n",
    "\n",
    "\n",
    "    file_path = os.path.join(output_path, file_name)\n",
    "\n",
    "    # 결과를 파일로 저장\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Question: {query}\\n\")\n",
    "        f.write(f\"Distance: {average_score:.3f}\\n\")\n",
    "        f.write(\"Final Response:\\n\")\n",
    "        f.write(response.content)\n",
    "    \n",
    "    print(f\"Result saved to: {file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
